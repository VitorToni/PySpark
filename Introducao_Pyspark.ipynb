{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introducao_Pyspark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO7Cx0ZiZYYS0epnZNIOhkS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VitorToni/PySpark/blob/main/Introducao_Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dependências**"
      ],
      "metadata": {
        "id": "_srQpS1a3SMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instalar as dependências\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "J3H3QmjE3Sl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar as variáveis de ambiente\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "\n",
        "# Torna o pyspark \"importável\"\n",
        "import findspark\n",
        "findspark.init('spark-2.4.4-bin-hadoop2.7')\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkFiles\n",
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "5Hl3yPg2rFAq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Criar sessão e Importar CSV**"
      ],
      "metadata": {
        "id": "_6LRkpL83gDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# iniciar uma sessão local \n",
        "#sc = SparkSession.builder.master('local[*]').getOrCreate()\n",
        "spark = SparkSession.builder.appName(\"Introducao\").getOrCreate()"
      ],
      "metadata": {
        "id": "rGVVtpoDrVWY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark)\n",
        "print(spark.version)"
      ],
      "metadata": {
        "id": "GyLYdcowrin1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "#files.upload()"
      ],
      "metadata": {
        "id": "FGswOva0rq0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adicionando csv ao Spark\n",
        "# Obs:  Csv's usados são de repositórios públicos, caso não funcione, procure outra fonte (Não deve ser dificl, dado que estas bases são muito utilizadas pela comunidade)\n",
        "\n",
        "spark.sparkContext.addFile('https://raw.githubusercontent.com/roberthryniewicz/datasets/master/airline-dataset/flights/flights.csv')\n",
        "\n",
        "flights = spark.read.csv(SparkFiles.get(\"flights.csv\"), \n",
        "               inferSchema = True,\n",
        "               header = True)"
      ],
      "metadata": {
        "id": "p9L9ZGYluVtQ"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark.catalog.listTables())"
      ],
      "metadata": {
        "id": "udbX3bFqzYkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((flights.count(), len(flights.columns)))"
      ],
      "metadata": {
        "id": "kAt2rw9pu5UM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flights.show(10)"
      ],
      "metadata": {
        "id": "HkcWTARIvHPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flights.printSchema()"
      ],
      "metadata": {
        "id": "GUt8iqq5vIDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Criando Views e manipulando dados**"
      ],
      "metadata": {
        "id": "YrwBbIVs4aDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Corrigindo formato da coluna\n",
        "\n",
        "flights = flights.\\\n",
        "        withColumn(\"new_air_time\", col(\"AirTime\").cast(\"integer\")).drop(\"AirTime\").\\\n",
        "        withColumnRenamed(\"new_air_time\",\"AirTime\")"
      ],
      "metadata": {
        "id": "ilWmzC0x4q9M"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Registrando o dataframe em uma view temporária\n",
        "flights.createOrReplaceTempView(\"flights_v\")\n",
        "\n",
        "# Registtrando o dataframe como view global\n",
        "flights.createGlobalTempView(\"flights_gv\")\n",
        "\n",
        "# A visão temporária global está vinculada a um banco de dados preservado pelo sistema `global_temp`\n",
        "flight_counts = spark.sql(\"SELECT Origin, Dest, COUNT(*) as N FROM global_temp.flights_gv GROUP BY 1, 2\")\n",
        "\n",
        "# Transformando em uma DF\n",
        "df = flight_counts.toPandas()\n",
        "df"
      ],
      "metadata": {
        "id": "K9UYkO0QwQPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria uma coluna com o resultado de uma operação\n",
        "flights = flights.withColumn(\"duration_hrs\", flights.AirTime / 60)"
      ],
      "metadata": {
        "id": "F63YqtNnxkow"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Selecting**"
      ],
      "metadata": {
        "id": "y_4tYkF14xu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Formas de Select\n",
        "flights.select(flights.AirTime / 60).show()\n",
        "# Poderia usar o ALIAS para renomear no select:\n",
        "#flights.select((flights.AirTime / 60).alias(\"duration_hrs\")).show()\n",
        "\n",
        "flights.select(\"AirTime\", \"duration_hrs\").show()\n",
        "\n",
        "lista = [\"AirTime\", \"duration_hrs\"]\n",
        "flights.select(lista).show()\n",
        "\n",
        "flights.select(flights.AirTime, flights.duration_hrs).show()\n",
        "\n",
        "# Pode usar o show(5) para limitar o retorno"
      ],
      "metadata": {
        "id": "8p2PfD6L1KeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Filtrando**\n",
        "\n",
        "`.filter()` é uma contrapartida do Spark da cláusula `WHERE` do `SQL`."
      ],
      "metadata": {
        "id": "-jfoME3c4_9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flights.filter(\"AirTime > 120\").show(5)\n",
        "#flights.filter(flights.AirTime > 120).show(5)\n",
        "\n",
        "flights.filter(\"AirTime >= 360 and Year = 2008\").show()"
      ],
      "metadata": {
        "id": "5YXo6_Ff489E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filterA = flights.Origin == \"PVD\"\n",
        "filterB = flights.Dest == \"LAS\"\n",
        "\n",
        "# Filtrando de forma sequencial\n",
        "flights.filter(filterA).filter(filterB).show(5)"
      ],
      "metadata": {
        "id": "1yAEWAot6xk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Agregando**"
      ],
      "metadata": {
        "id": "vGdwDiwv8fif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Achar a maior tempo de voo de SEA para outras cidades\n",
        "flights.filter(flights.Origin == \"SEA\").groupBy().max(\"duration_hrs\").show()\n",
        "\n",
        "# Achar a menor distancia do voo de PDX para outras cidades\n",
        "flights.filter(flights.Origin == \"PDX\").groupBy().min(\"distance\").show()\n",
        "\n",
        "# Duração Média dos Voos da compania delta\n",
        "flights.filter(flights.UniqueCarrier == \"XE\").filter(flights.Origin == \"DAY\").groupBy().avg(\"AirTime\").show()\n",
        "\n",
        "# Tempo total em Horas no ar \n",
        "flights.groupBy().sum(\"duration_hrs\").show()\n",
        "# Caso deseje renomear:\n",
        "#flights.groupBy().sum(\"duration_hrs\")\\\n",
        "#  .withColumnRenamed(\"sum(duration_hrs)\", \"soma duration_hrs\").show()"
      ],
      "metadata": {
        "id": "F-tx8XlR8iJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Join**\n",
        "\n",
        "Parâmetros do `Join`:\n",
        "\n",
        "*   Tabela que deseja juntar;\n",
        "*   `On`, uma ou mais colunas;\n",
        "*   Tipo de `Join`, por padrão é `Inner`, porém existe os seguintes tipos:  `inner, cross, outer, full, fullouter, full_outer, left, leftouter, left_outer, right, rightouter, right_outer, semi, leftsemi, left_semi, anti, leftantie left_anti.`"
      ],
      "metadata": {
        "id": "WLyEOggwDicK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obs:  Csv's usados são de repositórios públicos, caso não funcione, procure outra fonte (Não deve ser dificl, dado que estas bases são muito utilizadas pela comunidade)\n",
        "spark.sparkContext.addFile('https://raw.githubusercontent.com/tidyverse/nycflights13/main/data-raw/airports.csv')\n",
        "\n",
        "airports = spark.read.csv(SparkFiles.get(\"airports.csv\"), \n",
        "               inferSchema = True,\n",
        "               header = True)\\\n",
        "               .drop(\"tzone\")\\\n",
        "               .withColumnRenamed(\"faa\", \"Dest\")"
      ],
      "metadata": {
        "id": "B1AhSdNVDkwt"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Juntando as tabelas para trazer as informações do aeroporto de destino\n",
        "\n",
        "flights_with_airports = flights.join(airports, on=\"Dest\", how=\"left\")\n",
        "flights_with_airports.show(5)"
      ],
      "metadata": {
        "id": "2BQUeRBpERof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# O mesmo exemplo, porém inseriando alias, deixando as condições mais claras e selecionando as colunas desejadas\n",
        "\n",
        "flights.alias(\"a\").join(airports.alias(\"b\"), \n",
        "                        col('a.Dest') == col('b.Dest'), \n",
        "                        \"left\")\\\n",
        "  .select(flights[\"*\"], airports[\"name\"])\\\n",
        "  .show(5)"
      ],
      "metadata": {
        "id": "6C3bo-hjHHCu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}